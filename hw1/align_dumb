#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
from random import random, choice

optparser = optparse.OptionParser()
optparser.add_option("-b", "--bitext", dest="bitext", default="data/dev-test-train.de-en", help="Parallel corpus (default data/dev-test-train.de-en)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

### Dumber IBM Model 1 ###

NUM_ITERATIONS = 5
NUM_TRAINING_DATA = 1000 # sys.maxint
NONE_INS = False # Whether to align w/ null token as well

def read_bitext(file, num=sys.maxint):
    return [[sentence.strip().split() for sentence in pair.split(' ||| ')]
                for pair in open(file)][:num]

def uniform_parameters(p=0.25):
    return defaultdict(lambda: p)

def random_parameters():
    return defaultdict(random)

def best_alignment(e, f, params):
    a = []
    for e_i in e:
        probs = [params[(e_i, f_j)] for f_j in f]
        a.append(choice([j for j, p in enumerate(probs) if p == max(probs)]))
    return a

def mean_len(x):
    return (len(x[0]) + len(x[1])) / 2.0

def em_iteration(params, bitext):
    ef_counts = defaultdict(float) # Avoid integer division problems.
    f_counts = defaultdict(float)
    for (f, e) in bitext:
        for e_i in e:
            for f_j in f:
                delta = params[(e_i,f_j)]/sum([params[(e_i,_f)] for _f in f])
                ef_counts[(e_i, f_j)] += delta
                f_counts[f_j] += delta
                params[(e_i, f_j)] = ef_counts[(e_i, f_j)] / f_counts[f_j]
    return params

# Implementation of alignment using Dumber IBM Model 1.
sys.stderr.write("Training with Dumber IBM Model 1...\n")
bitext = read_bitext(opts.bitext, opts.num_sents)
train = sorted(bitext[:NUM_TRAINING_DATA], key=mean_len, reverse=True)

# Parameters: Expected p(e_i | f_{a_i}).
params = uniform_parameters()
if NONE_INS:
    for (f, e) in bitext:
        f.insert(0, None)
for k in range(NUM_ITERATIONS):
    sys.stderr.write("Training model with iteration %i of EM algorithm.\n" % k)
    params = em_iteration(params, train)
sys.stderr.write("Finished training model.\n")

for (f, e) in bitext:
    a = best_alignment(e, f, params)
    for (i, a_i) in enumerate(a):
        if a_i > 0 and NONE_INS:
            sys.stdout.write("%i-%i " % (a_i-1, i))
        else:
            sys.stdout.write("%i-%i " % (a_i, i))
    sys.stdout.write("\n")
sys.stderr.write("Finished writing output.\n")
