DIAG w/ memoizing, real best probs
1K      diag_1K2.al     4m27.524s   0.421609
100K    diag_100K1.al   17m6.084s   0.341798
1K      diag_1K3.al     4m22.502s   0.417508
1K      diag_1K4.al     4m22.958s   0.415615
1K      diag_1K5.al     4m25.507s   0.438486
1K      diag_1K6.al     4m32.954s   0.417035
100K    diag_100K2.al   16m31.994s  0.331230
1K      diag_1K7.al     4m32.612s   0.415615

Lambda = 1.5 for diag_1K3.al.
Lambda = 2.0 for diag_1K4.al.
Lambda = 0.5 for diag_1K5.al.
Lambda = 1.5, P0 = 0.1, InsNone = True for diag_1K6.al.
Lambda = 4.0, P0 = 0.08, InsNone = True for diag_1K7.al.
Lambda = 1.5, P0 = 0.0 for diag_100K2.al.
These were fixed in Chris's paper, so no point training them.
diag_1K7.al were his recommended parameters.
The results are identical to Lambda = 2.0 and no P0.
That means we'll leave it as is or do a full run with L=2.0.

Now let's try IBM2.
100K    ibm2_100K1.al   19m28.876s  0.357098

Quite good, but need a way to work in DIAG to make it worthwhile.


Pending that, let's mess with EXP
1K      exp_1K2.al      35m23.003s  0.555521
1K      exp_1K3.al      31m5.307s   0.574448
1K      exp_1K4.al      6m22.594s   0.590536

exp_1K2.al: Threshold = 0.5, Min phrase = 2; 253011 new
exp_1K3.al: Threshold = 0.75, Min phrase = 2; 78381 new
exp_1K4.al: Ditto, w/ RegWeight = 0.25

No good.  There's a slight chance it works better with IBM2.
Plus it looks like emphasizing the "good" ones makes it worse.


Combining DIAG with IBM2:
1K      diag2_1K1.al    N/A         0.457413
10K     diag2_10K1.al   7m53.053s   0.411514
1K      diag2_1K2.al    4m54.992s   0.455205
100K    diag2_100K1.al  23m2.433s   0.324763*

diag2_1K1.al: Lambda = 2.0, P0 = 0.0
diag2_10K1.al: Lambda = 4.0, P0 = 0.08, InsNone = True
diag1_1K2.al: Lambda = 4.0, P0 = 0.08, InsNone = True, InitDiag = True

DIAG2 doesn't seem to work as well.
Maybe the diag keeps the alignment training from helping.
I'll try the diagonal as just the initial distribution.
This is tested in diag1_1K2.al.
This doesn't seem to have helped.
I'll try a full run just for fun.
Hey, it snuck past diag's 100K run by half a point.
Time to upload it.

Next up: Markov assumption on the alignments instead of pure independence.
1K      markov_1K1.al   5m24.057s   0.447792
100K    markov_100K1.al N/A         N/A

Had to smooth zero-denominator probs to 0.01.
But memory blossoms to 12.2G after one round of EM.
We're going to need to limit that somehow.
One approach might be to skip the dependency on j in favor of one on a_prev.

Let's try one of these with params initialized to something smarter.
How about 0.6 - 0.5 * abs(len(f) - len(e)) / max(len(f), len(e))

1K      diag2_1K3.al    5m33.728s   0.432334
100K    diag2_100K2.al  25m55.707s  0.322397(*)
100K    diag2_100K3.al  

That's a slightly better result.  Time for a full run.
Again, that's a very slightly better result.
Probably not worth a new upload.
I'm going to up the count to 10 for diag2_100K3.al.
