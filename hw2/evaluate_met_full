#!/usr/bin/env python
import argparse # optparse is deprecated
from itertools import islice # slicing for iterators
from collections import Counter


from nltk.corpus import wordnet as wn
from nltk.corpus import wordnet_ic as wn_ic
from nltk.stem import porter
wn.ensure_loaded()
brown_ic = wn_ic.ic('ic-brown.dat')
semcor_ic = wn_ic.ic('ic-semcor.dat')
ps = porter.PorterStemmer()

import os
from nltk.parse import stanford
os.environ['STANFORD_PARSER'] = '/home/constantine/stanford-parser-full-2015-01-30'
os.environ['STANFORD_MODELS'] = '/home/constantine/stanford-parser-full-2015-01-30'

sp = stanford.StanfordParser(model_path='/home/constantine/stanford-parser-full-2015-01-30/edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')

# wn.path_similarity(x,y)
# wn.lch_similarity(x,y)
# wn.wup_similarity(x,y)
# x.res_similarity(y, brown_ic)
# x.jcn_similarity(y, brown_ic)

# wn.lin_similarity(?,?,?,?)
# wn.ic()
# nltk.corpus.wordnet_ic.ic('ic-brown.dat')
# wn.res_similarity(?,?)

def har_mean(p, r, a):
    den = (a * p + (1 - a) * r)
    if den == 0: return 0.0
    return p * r / den

def simple_meteor(h, ref, alpha):
    m = float(sum((Counter(h) & Counter(ref)).viewvalues()))
    p = m / len(h)
    r = m / len(ref)
    return har_mean(p, r, alpha)

def exact_match(x, y):
    return x == y

def stem_match(x, y):
    try:
        return ps.stem(unicode(x)) == ps.stem(unicode(y))
    except UnicodeDecodeError as e:
        return False

def syn_match(x, y):
    try:
        return len(set(wn.synsets(x)) & set(wn.synsets(y))) > 0
    except UnicodeDecodeError as e:
        return False

EXACT_WEIGHT = 1.0
STEM_WEIGHT = 1.0
SYN_WEIGHT = 1.0

def eq_loop(f, w, h, ref, h_mark, ref_mark):
    for i, h_word in enumerate(h):
        if h_mark[i] > 0: continue
        for j, ref_word in enumerate(ref):
            if ref_mark[j] > 0: continue
            match = f(h_word, ref_word)
            h_mark[i] += w if match else 0
            ref_mark[j] += w if match else 0

def full_meteor(h, ref, alpha):
    h_mark = [0.0] * len(h)
    ref_mark = [0.0] * len(ref)
    eq_loop(exact_match, EXACT_WEIGHT, h, ref, h_mark, ref_mark)
    eq_loop(stem_match, STEM_WEIGHT, h, ref, h_mark, ref_mark)
    eq_loop(syn_match, SYN_WEIGHT, h, ref, h_mark, ref_mark)
    m = sum(h_mark)
    p = m / len(h)
    r = m / len(ref)
    return har_mean(p, r, alpha)
 
def main():
    parser = argparse.ArgumentParser(description='Evaluate translation hypotheses.')
    # PEP8: use ' and not " for strings
    parser.add_argument('-i', '--input', default='data/train-test.hyp1-hyp2-ref',
            help='input file (default data/train-test.hyp1-hyp2-ref)')
    parser.add_argument('-n', '--num_sentences', default=None, type=int,
            help='Number of hypothesis pairs to evaluate')
    parser.add_argument('-a', '--alpha', default=0.5, type=float, help='meteor weight')
    # note that if x == [1, 2, 3], then x[:None] == x[:] == x (copy); no need for sys.maxint
    opts = parser.parse_args()
 
    # we create a generator and avoid loading all sentences into a list
    def sentences():
        with open(opts.input) as f:
            for pair in f:
                yield [sentence.strip().split() for sentence in pair.split(' ||| ')]
 
    # note: the -n option does not work in the original code
    for h1, h2, ref in islice(sentences(), opts.num_sentences):
        h1_val = full_meteor(h1, ref, opts.alpha)
        h2_val = full_meteor(h2, ref, opts.alpha)
        print(-1 if h1_val > h2_val else # \begin{cases}
                (0 if h1_val == h2_val
                    else 1)) # \end{cases}
 
# convention to allow import of this file as a module
if __name__ == '__main__':
    main()
