#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
from random import random, choice

optparser = optparse.OptionParser()
optparser.add_option("-b", "--bitext", dest="bitext", default="data/dev-test-train.de-en", help="Parallel corpus (default data/dev-test-train.de-en)")
optparser.add_option("-t", "--threshold", dest="threshold", default=0.5, type="float", help="Threshold for aligning with Dice's coefficient (default=0.5)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=sys.maxint, type="int", help="Number of sentences to use for training and alignment")
(opts, _) = optparser.parse_args()

# Implementation of alignment using IBM Model 2.
sys.stderr.write("Training with IBM Model 2...\n")
bitext = [[[x.lower() for x in sentence.strip().split()] for sentence in pair.split(' ||| ')] for pair in open(opts.bitext)][:opts.num_sents]

### IBM Model 2 ###

NUM_ITERATIONS = 5
NUM_TRAINING_DATA = 100000 # sys.maxint
NONE_INS = False # Whether to align w/ null token as well

def uniform_parameters(p=0.25):
    return defaultdict(lambda: p)

def random_parameters():
    return defaultdict(random)

def best_alignment(e, f, params):
    words, align = params
    a = []
    l, m = len(f), len(e)
    for i, e_i in enumerate(e):
        probs = [words[(e_i, f_j)] * align[(i, j, l, m)] for j, f_j in enumerate(f)]
        a.append(choice([j for j, p in enumerate(probs) if p == max(probs)]))
    return a

def em_iteration(params, bitext):
    words, align = params
    ef_counts = defaultdict(float) # Avoid integer division problems.
    f_counts = defaultdict(float)
    ilm_counts = defaultdict(float)
    ijlm_counts = defaultdict(float)
    for (f, e) in bitext:
        l, m = len(f), len(e)
        for i, e_i in enumerate(e):
            for j, f_j in enumerate(f):
                delta = align[(i, j, l, m)] * words[(e_i,f_j)]/sum([align[(i, _j, l, m)] * words[(e_i,_f)] for _j, _f in enumerate(f)])
                ef_counts[(e_i, f_j)] += delta
                f_counts[f_j] += delta
                ilm_counts[(i, l, m)] += delta
                ijlm_counts[(i, j, l, m)] += delta
    
    for (e, f) in words: # set(params.keys()) | set(ef_counts.keys()):
        words[(e, f)] = ef_counts[(e, f)] / f_counts[f]
    for (i, j, l, m) in align:
        align[(i, j, l, m)] = ijlm_counts[(i, j, l, m)] / ilm_counts[(i, l, m)]
    return (words, align)

# Parameters: Expected p(e_i | f_{a_i}).
words, align = uniform_parameters(), uniform_parameters()
params = (words, align)
if NONE_INS:
    for (f, e) in bitext:
        f.insert(0, None)
for k in range(NUM_ITERATIONS):
    sys.stderr.write("Training model with iteration %i of EM algorithm.\n" % k)
    params = em_iteration(params, bitext[:NUM_TRAINING_DATA])
sys.stderr.write("Finished training model.\n")

for (f, e) in bitext:
    a = best_alignment(e, f, params)
    for (i, a_i) in enumerate(a):
        if a_i > 0 and NONE_INS:
            sys.stdout.write("%i-%i " % (a_i-1, i))
        else:
            sys.stdout.write("%i-%i " % (a_i, i))
    sys.stdout.write("\n")
sys.stderr.write("Finished writing output.\n")
